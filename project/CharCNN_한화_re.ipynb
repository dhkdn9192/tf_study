{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_NAME = '한화'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from MeCab import Tagger\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as ktf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D, Concatenate, Input, Embedding, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 설정\n",
    "def get_session():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5,allow_growth=True,visible_device_list='2')\n",
    "    return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "ktf.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "seq_size = 70  # model input shape[0], 입력 데이터의 형태소 최대 갯수\n",
    "embed_size = 128  # model input shape[1], 각 형태소의 임베딩 벡터 크기\n",
    "batch_size = 32 # 각 미니배치의 크리\n",
    "vocab_size = 455001  # word2index_dict의 단어 수\n",
    "validation_split = 0.1  # 학습 시 train set에서 validation set으로 사용할 데이터 비율\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "kernel_sizes = [3, 4, 5]  # kernel_size list for each CNN layers\n",
    "n_class = 1  # 분류할 클래스 수 (binary_crossentropy를 쓰므로 1개 클래스가 0 또는 1의 값을 가지는 것으로 2 클래스 분류)\n",
    "n_epoch = 13\n",
    "n_patience = 6  # early stop 콜백의 파라미터\n",
    "\n",
    "# random seed\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "root_path = '/data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification'\n",
    "data_path = f'{root_path}/data/sentences/increased_labeled/labeled_{STOCK_NAME}.txt'\n",
    "vocab_path = f'{root_path}/data/vocabulary/word2index_dict_190117.pkl'\n",
    "mecab_path = '-d /data/lib/mecab/mecab-ko-dic-default'\n",
    "\n",
    "# 저장경로용 시간, 파일명 문자열\n",
    "now_dt = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "model_name = data_path.split('/')[-1].replace('labeled_', '').replace('.txt', '')\n",
    "\n",
    "# 텐서보드 디렉토리 생성 및 로그 경로\n",
    "tensorboard_dir = 'tensorboard'\n",
    "if not os.path.exists(tensorboard_dir):\n",
    "    os.makedirs(tensorboard_dir)\n",
    "tblog_path = f'{root_path}/{tensorboard_dir}/{model_name}'\n",
    "\n",
    "# 체크포인트 디렉토리 생성\n",
    "ckp_dir = f'{root_path}/checkpoint/{model_name}/'\n",
    "if not os.path.exists(ckp_dir):\n",
    "    os.makedirs(ckp_dir)\n",
    "ckp_path = os.path.join(ckp_dir, now_dt + '_weights.{epoch:03d}-{val_acc:.4f}.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어사전 로드\n",
    "word2index_dict.pkl을 로드한다. 거의 대부분의 단어들을 유니크한 숫자로 인코딩하기 위한 딕셔너리이다. (str -> int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455001\n"
     ]
    }
   ],
   "source": [
    "with open(vocab_path, 'rb') as fp:\n",
    "    word2index_dict = pickle.load(fp)\n",
    "    \n",
    "if vocab_size != len(word2index_dict):\n",
    "    vocab_size = len(word2index_dict)\n",
    "print(len(word2index_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(data_path, 'r') as fp:\n",
    "        data = [l.strip() for l in fp.readlines() if len(l) > 10 and len(l.split())]\n",
    "    data_X = [d.rsplit('#', 1)[0] for d in data]\n",
    "    data_y = [int(d.rsplit('#', 1)[-1]) for d in data]\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_X size :  49267\n",
      "data_y size :  49267\n",
      "data_X[10] :  다이와넥스트은행의 설립 당시 자 본금은 500억엔(한화 4492억원)이었다\n",
      "data_y[10] :  0\n"
     ]
    }
   ],
   "source": [
    "data_X, data_y = load_data(data_path)\n",
    "print('data_X size : ', len(data_X))\n",
    "print('data_y size : ', len(data_y))\n",
    "print('data_X[10] : ', data_X[10])\n",
    "print('data_y[10] : ', data_y[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! 클래스별 데이터 수 비교하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_0_data size : 9561\n",
      "class_1_data size : 39706\n"
     ]
    }
   ],
   "source": [
    "# 레이블별 데이터 비율 비교\n",
    "class_0_data = [x for x, y in zip(data_X, data_y) if y == 0]\n",
    "class_1_data = [x for x, y in zip(data_X, data_y) if y == 1]\n",
    "\n",
    "print(f'class_0_data size : {len(class_0_data)}')\n",
    "print(f'class_1_data size : {len(class_1_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_x_c0 = class_0_data\n",
    "# sample_x_c1 = random.sample(class_1_data, 40000)\n",
    "# data_X = sample_x_c0 + sample_x_c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tagger = Tagger(mecab_path)\n",
    "    raw_tokens = tagger.parse(sentence).splitlines()\n",
    "    parsed_tokens = []\n",
    "    for raw_token in raw_tokens:\n",
    "        word_tag_ruple = raw_token.split('\\t')\n",
    "        if len(word_tag_ruple) != 2:\n",
    "            continue\n",
    "        word_stem = word_tag_ruple[0]\n",
    "        word_tag = word_tag_ruple[1].split(',')[0]\n",
    "        if not word_stem or not word_tag:\n",
    "            continue\n",
    "        if word_tag in {'NNP', 'NNG', 'SL', 'VV', 'VA', 'VX'}:\n",
    "            parsed_tokens.append(word_stem)\n",
    "    return parsed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 49267/49267 [00:08<00:00, 5659.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_sents size :  49267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_sents = []\n",
    "with Pool(processes=8) as pool:\n",
    "    for tokens in tqdm(pool.imap(tokenize, data_X), total=len(data_X), desc='tokenizing'):\n",
    "        if tokens:\n",
    "            tokenized_sents.append(tokens)\n",
    "            \n",
    "print('tokenized_sents size : ', len(tokenized_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens):\n",
    "    padded_tokens = list(map(lambda x : tokens[x] if x < len(tokens) else '#', range(seq_size)))\n",
    "    embed_vect = list(map(lambda w : word2index_dict[w] if w in word2index_dict else 0, padded_tokens))\n",
    "    return embed_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 100%|██████████| 49267/49267 [00:03<00:00, 15087.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_X size :  49267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encode_X = []\n",
    "with Pool(processes=8) as pool:\n",
    "    for encoded_tokens in tqdm(pool.imap(encode, tokenized_sents), total=len(data_X), desc='encoding'):\n",
    "        encode_X.append(encoded_tokens)\n",
    "        \n",
    "print('encode_X size : ', len(encode_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train - Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(data_x, data_y):\n",
    "    \n",
    "    x_c0, x_c1 = [], []\n",
    "    for x, y in zip(data_x, data_y):\n",
    "        if y == 0:\n",
    "            x_c0.append(x)\n",
    "        if y == 1:\n",
    "            x_c1.append(x)\n",
    "    \n",
    "    increased_c1 = []\n",
    "    \n",
    "    if len(x_c0) >= len(x_c1)*2:\n",
    "        div_ratio = int(len(x_c0) / len(x_c1))\n",
    "        for i in range(div_ratio):\n",
    "            if  (i + 1) * len(x_c1) < len(x_c0):\n",
    "                increased_c1 += x_c1\n",
    "    else:\n",
    "        increased_c1 = x_c1\n",
    "            \n",
    "    shuff_c0 = random.sample(x_c0, len(x_c0))\n",
    "    shuff_c1 = random.sample(increased_c1, len(increased_c1))\n",
    "    \n",
    "    c0_tsize = int(len(shuff_c0) * validation_split)\n",
    "    c1_tsize = int(len(shuff_c1) * validation_split)\n",
    "    \n",
    "    test_X = shuff_c0[:c0_tsize] + shuff_c1[:c1_tsize]\n",
    "    train_X = shuff_c0[c0_tsize:] + shuff_c1[c1_tsize:]\n",
    "    test_y = [0 for _ in range(c0_tsize)] + [1 for _ in range(c1_tsize)]\n",
    "    train_y = [0 for _ in range(len(shuff_c0) - c0_tsize)] + [1 for _ in range(len(shuff_c1) - c1_tsize)]    \n",
    "        \n",
    "    return np.array(train_X), np.array(train_y), np.array(test_X), np.array(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X size : 44341\n",
      "train_y size : 44341\n",
      "test_X size : 4926\n",
      "test_y size : 4926\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = get_train_test_data(encode_X, data_y)\n",
    "\n",
    "print(f'train_X size : {len(train_X)}')\n",
    "print(f'train_y size : {len(train_y)}')\n",
    "print(f'test_X size : {len(test_X)}')\n",
    "print(f'test_y size : {len(test_y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_cnn_model(kernel_sizes, seq_size, vocab_size, embed_size, dropout_rate, n_class, learning_rate):\n",
    "    # input layer\n",
    "    inputs = Input(shape=(seq_size,), dtype='float32', name='input')\n",
    "    \n",
    "    embedded = Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_size, name='embedded')(inputs)\n",
    "    reshaped = Reshape((seq_size, embed_size, 1), name='reshape')(embedded)\n",
    "\n",
    "    # multiple CNN layers\n",
    "    convolution_layers = []\n",
    "    for idx, kernel_size in enumerate(kernel_sizes):\n",
    "        conv = Conv2D(filters=256, kernel_size=(kernel_size, embed_size), padding='valid', activation='relu', name=f'conv_{idx}')(reshaped)\n",
    "        pool = MaxPooling2D(pool_size=(seq_size - kernel_size + 1, 1), strides=(1, 1), padding='valid', name=f'pool_{idx}')(conv)\n",
    "        dropout = Dropout(rate=dropout_rate, name=f'dropout_{idx}')(pool)\n",
    "        convolution_layers.append(dropout)\n",
    "        \n",
    "    # concatenate all CNN output tensors\n",
    "    concat_tensor = Concatenate(axis=1, name='concat')(convolution_layers)    \n",
    "    \n",
    "    # middle fully-connected layer\n",
    "    flatten = Flatten(name='flatten')(concat_tensor)\n",
    "    hidden = Dense(384, activation='relu', name='hidden')(flatten)\n",
    "    dropout = Dropout(rate=dropout_rate, name='dropout')(hidden)\n",
    "    \n",
    "    # output layer\n",
    "    outputs = Dense(n_class, activation='sigmoid', name='output')(dropout)\n",
    "\n",
    "    # connect layers to model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # compile model\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedded (Embedding)            (None, 70, 128)      58240128    input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 70, 128, 1)   0           embedded[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv2D)                 (None, 68, 1, 256)   98560       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 67, 1, 256)   131328      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 66, 1, 256)   164096      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool_0 (MaxPooling2D)           (None, 1, 1, 256)    0           conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pool_1 (MaxPooling2D)           (None, 1, 1, 256)    0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pool_2 (MaxPooling2D)           (None, 1, 1, 256)    0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_0 (Dropout)             (None, 1, 1, 256)    0           pool_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 1, 256)    0           pool_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 1, 256)    0           pool_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 3, 1, 256)    0           dropout_0[0][0]                  \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 768)          0           concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "hidden (Dense)                  (None, 384)          295296      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 384)          0           hidden[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            385         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 58,929,793\n",
      "Trainable params: 58,929,793\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_char_cnn_model(kernel_sizes, seq_size, vocab_size, embed_size, dropout_rate, n_class, learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=n_patience)\n",
    "# tensorboard = TensorBoard(log_dir=tblog_path, batch_size=batch_size)\n",
    "checkpoint = ModelCheckpoint(ckp_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto', save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir=tblog_path, **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()\n",
    "        \n",
    "tensorboard = TrainValTensorBoard(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/venv/lib64/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44341 samples, validate on 4926 samples\n",
      "Epoch 1/13\n",
      "44341/44341 [==============================] - 37s 840us/step - loss: 0.1328 - acc: 0.9491 - val_loss: 0.0339 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03390, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/한화/20190308172117_weights.001-0.9898.hdf5\n",
      "Epoch 2/13\n",
      "44341/44341 [==============================] - 35s 793us/step - loss: 0.0332 - acc: 0.9892 - val_loss: 0.0256 - val_acc: 0.9905\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03390 to 0.02562, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/한화/20190308172117_weights.002-0.9905.hdf5\n",
      "Epoch 3/13\n",
      "44341/44341 [==============================] - 35s 791us/step - loss: 0.0205 - acc: 0.9932 - val_loss: 0.0268 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02562\n",
      "Epoch 4/13\n",
      "44341/44341 [==============================] - 35s 789us/step - loss: 0.0174 - acc: 0.9947 - val_loss: 0.0229 - val_acc: 0.9929\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02562 to 0.02288, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/한화/20190308172117_weights.004-0.9929.hdf5\n",
      "Epoch 5/13\n",
      "44341/44341 [==============================] - 35s 789us/step - loss: 0.0127 - acc: 0.9961 - val_loss: 0.0233 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02288\n",
      "Epoch 6/13\n",
      "44341/44341 [==============================] - 35s 783us/step - loss: 0.0097 - acc: 0.9971 - val_loss: 0.0253 - val_acc: 0.9921\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02288\n",
      "Epoch 7/13\n",
      "44341/44341 [==============================] - 35s 779us/step - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0264 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02288\n",
      "Epoch 8/13\n",
      "44341/44341 [==============================] - 34s 774us/step - loss: 0.0080 - acc: 0.9978 - val_loss: 0.0262 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02288\n",
      "Epoch 9/13\n",
      "44341/44341 [==============================] - 35s 781us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0260 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02288\n",
      "Epoch 10/13\n",
      "44341/44341 [==============================] - 34s 773us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0322 - val_acc: 0.9925\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02288\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_X, \n",
    "    train_y, \n",
    "    batch_size=batch_size, \n",
    "    epochs=n_epoch,\n",
    "    validation_data=(test_X, test_y),\n",
    "    callbacks=[early_stopping, tensorboard, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/venv/lib64/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('/data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/한화/20190308172117_weights.002-0.9905.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    encodes = encode(tokens)\n",
    "    tf_input = np.array(encodes).reshape((1, seq_size))\n",
    "    pred = model.predict(tf_input)[0][0]\n",
    "    result = 1 if pred >= 0.85 else 0\n",
    "    return result, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.99987507)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '한화에너지(대표이사 류두형)는 미국 하와이 전력청(HECO)이 주관한 태양광+에너지저장장치(ESS) 발전소 건설 및 운영사업 입찰'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9954542)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '[보안뉴스 박미영 기자] 한화는 지난 17일 종합연구소(대전 유성구 장동 소재)에서 직접 보고 들은 선진 방산 기술을 공유하는 글로벌 방산 기술 공유 자리를 마련했다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.99936086)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '한화테크윈은 “일상 곳곳의 안전보안 솔루션(Expansion of Secure Life)”을 주제로 전시부스를 체험형 부스로 기획했다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.002326496)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '탄탄해진 안방은 한화 이글스가 2018시즌 가을야구에 진출한 비결 중 하나였다. '\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.00033649872)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '2018년 11년 만의 가을야구 진출에 성공한 한화이글스가 팀 역사상 유일무이한 첫 번째 우승을 한 게 1999년이다.'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 7.1868286e-05)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '열심히 일한 직원들 위해 연말 보너스로 현금 500억 원 쏜 통 큰 사장님 ... 바로 3억 위안(한화 약 500억 원)에 달하는 현금을 연말 보너스로 직원들'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.00010316949)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '선수들은 월급 1,250달러(한화 약 140만 원)와 상금 5만 5,125달러(한화 약 6,216만 원)가 지급되지 않았다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.00036213495)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '하지만 여름 이적 시장을 통해 사파타가 1400만 유로(한화 약 180억)의 금액과 함께 아탈란타로 임대를 떠나면서 삼프도리아가 자랑하던 영혼'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9999974)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '한화 대전공장 폭발사고 희생자 유가족들이 김승연 한화그룹 회장과의 면담을 요구했지만 성사되지 않았다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9818885)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '유가족들은 8일 오전 서울 장교동 한화빌딩 앞에서 기자회견을 열고 회사의 무책임한 태도를 규탄했다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9275667)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '기자회견 직후 유가족들은 김승연 회장과의 면담을 위해 한화빌딩 1층 로비로 진입했다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.27977183)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '이에 최선목 한화커뮤니케이션위원회 위원장은 유가족들에게 \"대화할 수 있는 회의실을 준비했다\"며 \"김승연 회장은 이 사안에 관여할 권한이 없다\"고 설명했다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.99999917)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '협상에는 유가족 대표 3명과 옥경석 한화 화약방산부문 대표이사, 최선목 위원장 총 5명이 나선다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.021755055)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '유가족들이 김 회장과의 면담을 요구하며 강하게 반발하고 나선 이유는 한화 측의 작업 매뉴얼 조작 의혹 때문이다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.43083677)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"한화제약(사장 김경락)이 급·만성호흡기질환 치료제 '뮤테란'의 시럽 제형을 이달 2일 출시했다고 밝혔다\"\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.018281424)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '뮤테란은 점액의 분비 운동력을 높여 화농성과 농성 점액 분비물의 점도를 최대한 낮춰 계면 활성층을 보강해 원활한 분비조절 작용을 촉진시키며, 점액의 수송 속도와 섬모운동의 횟수를 증가시키고, 기도의 청정화로 기침을 완화, 점액과 섬모계의 생리적 방어기능을 회복시켜 점막기능이 정상화 될 수 있도록 도와준다고 한화제약은 설명했다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0010890788)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '메니피’는 1998년부터 1년간 미국에서 경주마로 활동할 당시 11번의 경주를 거쳐 약 173만 미국달러(한화 약 19억 원)를 벌었지만, 씨수말로 전환 후 자마들의 상금 총합이 574억 원으로 놀라운 부가 가치를 창출하고 있다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3.656097e-06)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '나홀로집에’ 케인이 사용한 금액 967달러는 한화라면 100만원도 넘는 큰 금액이다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.070523284)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '그렇다면 2000년 ~ 2018년 시즌까지 체결된 FA 계약 201회 중에 원 소속팀에 남은 선수는 어느 정도나 될까'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
