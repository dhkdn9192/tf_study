{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_NAME = '카카오'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from MeCab import Tagger\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as ktf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D, Concatenate, Input, Embedding, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 설정\n",
    "def get_session():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5,allow_growth=True,visible_device_list='1')\n",
    "    return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "ktf.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "seq_size = 70  # model input shape[0], 입력 데이터의 형태소 최대 갯수\n",
    "embed_size = 128  # model input shape[1], 각 형태소의 임베딩 벡터 크기\n",
    "batch_size = 32 # 각 미니배치의 크리\n",
    "vocab_size = 455001  # word2index_dict의 단어 수\n",
    "validation_split = 0.1  # 학습 시 train set에서 validation set으로 사용할 데이터 비율\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "kernel_sizes = [3, 4, 5]  # kernel_size list for each CNN layers\n",
    "n_class = 1  # 분류할 클래스 수 (binary_crossentropy를 쓰므로 1개 클래스가 0 또는 1의 값을 가지는 것으로 2 클래스 분류)\n",
    "n_epoch = 20\n",
    "n_patience = 6  # early stop 콜백의 파라미터\n",
    "\n",
    "# random seed\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "root_path = '/data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification'\n",
    "data_path = f'{root_path}/data/sentences/increased_labeled/labeled_{STOCK_NAME}.txt'\n",
    "vocab_path = f'{root_path}/data/vocabulary/word2index_dict_190117.pkl'\n",
    "mecab_path = '-d /data/lib/mecab/mecab-ko-dic-default'\n",
    "\n",
    "# 저장경로용 시간, 파일명 문자열\n",
    "now_dt = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "model_name = data_path.split('/')[-1].replace('labeled_', '').replace('.txt', '')\n",
    "\n",
    "# 텐서보드 디렉토리 생성 및 로그 경로\n",
    "tensorboard_dir = 'tensorboard'\n",
    "if not os.path.exists(tensorboard_dir):\n",
    "    os.makedirs(tensorboard_dir)\n",
    "tblog_path = f'{root_path}/{tensorboard_dir}/{model_name}'\n",
    "\n",
    "# 체크포인트 디렉토리 생성\n",
    "ckp_dir = f'{root_path}/checkpoint/{model_name}/'\n",
    "if not os.path.exists(ckp_dir):\n",
    "    os.makedirs(ckp_dir)\n",
    "ckp_path = os.path.join(ckp_dir, now_dt + '_weights.{epoch:03d}-{val_acc:.4f}.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어사전 로드\n",
    "word2index_dict.pkl을 로드한다. 거의 대부분의 단어들을 유니크한 숫자로 인코딩하기 위한 딕셔너리이다. (str -> int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455001\n"
     ]
    }
   ],
   "source": [
    "with open(vocab_path, 'rb') as fp:\n",
    "    word2index_dict = pickle.load(fp)\n",
    "    \n",
    "if vocab_size != len(word2index_dict):\n",
    "    vocab_size = len(word2index_dict)\n",
    "print(len(word2index_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(data_path, 'r') as fp:\n",
    "        data = [l.strip() for l in fp.readlines() if len(l) > 10 and len(l.split())]\n",
    "    data_X = [d.rsplit('#', 1)[0] for d in data]\n",
    "    data_y = [int(d.rsplit('#', 1)[-1]) for d in data]\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_X size :  8040\n",
      "data_y size :  8040\n",
      "data_X[10] :  이들은 마약 성분으로 유명한 코카 대신 초콜릿 원료인 카카오를 키우고 있다\n",
      "data_y[10] :  0\n"
     ]
    }
   ],
   "source": [
    "data_X, data_y = load_data(data_path)\n",
    "print('data_X size : ', len(data_X))\n",
    "print('data_y size : ', len(data_y))\n",
    "print('data_X[10] : ', data_X[10])\n",
    "print('data_y[10] : ', data_y[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! 클래스별 데이터 수 비교하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_0_data size : 3270\n",
      "class_1_data size : 4770\n"
     ]
    }
   ],
   "source": [
    "# 레이블별 데이터 비율 비교\n",
    "class_0_data = [x for x, y in zip(data_X, data_y) if y == 0]\n",
    "class_1_data = [x for x, y in zip(data_X, data_y) if y == 1]\n",
    "\n",
    "print(f'class_0_data size : {len(class_0_data)}')\n",
    "print(f'class_1_data size : {len(class_1_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_x_c0 = class_0_data\n",
    "# sample_x_c1 = random.sample(class_1_data, 40000)\n",
    "# data_X = sample_x_c0 + sample_x_c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tagger = Tagger(mecab_path)\n",
    "    raw_tokens = tagger.parse(sentence).splitlines()\n",
    "    parsed_tokens = []\n",
    "    for raw_token in raw_tokens:\n",
    "        word_tag_ruple = raw_token.split('\\t')\n",
    "        if len(word_tag_ruple) != 2:\n",
    "            continue\n",
    "        word_stem = word_tag_ruple[0]\n",
    "        word_tag = word_tag_ruple[1].split(',')[0]\n",
    "        if not word_stem or not word_tag:\n",
    "            continue\n",
    "        if word_tag in {'NNP', 'NNG', 'SL', 'VV', 'VA', 'VX'}:\n",
    "            parsed_tokens.append(word_stem)\n",
    "    return parsed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 8040/8040 [00:01<00:00, 5649.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_sents size :  8040\n"
     ]
    }
   ],
   "source": [
    "tokenized_sents = []\n",
    "with Pool(processes=8) as pool:\n",
    "    for tokens in tqdm(pool.imap(tokenize, data_X), total=len(data_X), desc='tokenizing'):\n",
    "        if tokens:\n",
    "            tokenized_sents.append(tokens)\n",
    "            \n",
    "print('tokenized_sents size : ', len(tokenized_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens):\n",
    "    padded_tokens = list(map(lambda x : tokens[x] if x < len(tokens) else '#', range(seq_size)))\n",
    "    embed_vect = list(map(lambda w : word2index_dict[w] if w in word2index_dict else 0, padded_tokens))\n",
    "    return embed_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 100%|██████████| 8040/8040 [00:00<00:00, 14093.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_X size :  8040\n"
     ]
    }
   ],
   "source": [
    "encode_X = []\n",
    "with Pool(processes=8) as pool:\n",
    "    for encoded_tokens in tqdm(pool.imap(encode, tokenized_sents), total=len(data_X), desc='encoding'):\n",
    "        encode_X.append(encoded_tokens)\n",
    "        \n",
    "print('encode_X size : ', len(encode_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train - Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(data_x, data_y):\n",
    "    \n",
    "    x_c0, x_c1 = [], []\n",
    "    for x, y in zip(data_x, data_y):\n",
    "        if y == 0:\n",
    "            x_c0.append(x)\n",
    "        if y == 1:\n",
    "            x_c1.append(x)\n",
    "    \n",
    "    increased_c1 = []\n",
    "    \n",
    "    if len(x_c0) >= len(x_c1)*2:\n",
    "        div_ratio = int(len(x_c0) / len(x_c1))\n",
    "        for i in range(div_ratio):\n",
    "            if  (i + 1) * len(x_c1) < len(x_c0):\n",
    "                increased_c1 += x_c1\n",
    "    else:\n",
    "        increased_c1 = x_c1\n",
    "            \n",
    "    shuff_c0 = random.sample(x_c0, len(x_c0))\n",
    "    shuff_c1 = random.sample(increased_c1, len(increased_c1))\n",
    "    \n",
    "    c0_tsize = int(len(shuff_c0) * validation_split)\n",
    "    c1_tsize = int(len(shuff_c1) * validation_split)\n",
    "    \n",
    "    test_X = shuff_c0[:c0_tsize] + shuff_c1[:c1_tsize]\n",
    "    train_X = shuff_c0[c0_tsize:] + shuff_c1[c1_tsize:]\n",
    "    test_y = [0 for _ in range(c0_tsize)] + [1 for _ in range(c1_tsize)]\n",
    "    train_y = [0 for _ in range(len(shuff_c0) - c0_tsize)] + [1 for _ in range(len(shuff_c1) - c1_tsize)]    \n",
    "        \n",
    "    return np.array(train_X), np.array(train_y), np.array(test_X), np.array(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X size : 7236\n",
      "train_y size : 7236\n",
      "test_X size : 804\n",
      "test_y size : 804\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = get_train_test_data(encode_X, data_y)\n",
    "\n",
    "print(f'train_X size : {len(train_X)}')\n",
    "print(f'train_y size : {len(train_y)}')\n",
    "print(f'test_X size : {len(test_X)}')\n",
    "print(f'test_y size : {len(test_y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_cnn_model(kernel_sizes, seq_size, vocab_size, embed_size, dropout_rate, n_class, learning_rate):\n",
    "    # input layer\n",
    "    inputs = Input(shape=(seq_size,), dtype='float32', name='input')\n",
    "    \n",
    "    embedded = Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_size, name='embedded')(inputs)\n",
    "    reshaped = Reshape((seq_size, embed_size, 1), name='reshape')(embedded)\n",
    "\n",
    "    # multiple CNN layers\n",
    "    convolution_layers = []\n",
    "    for idx, kernel_size in enumerate(kernel_sizes):\n",
    "        conv = Conv2D(filters=256, kernel_size=(kernel_size, embed_size), padding='valid', activation='relu', name=f'conv_{idx}')(reshaped)\n",
    "        pool = MaxPooling2D(pool_size=(seq_size - kernel_size + 1, 1), strides=(1, 1), padding='valid', name=f'pool_{idx}')(conv)\n",
    "        dropout = Dropout(rate=dropout_rate, name=f'dropout_{idx}')(pool)\n",
    "        convolution_layers.append(dropout)\n",
    "        \n",
    "    # concatenate all CNN output tensors\n",
    "    concat_tensor = Concatenate(axis=1, name='concat')(convolution_layers)    \n",
    "    \n",
    "    # middle fully-connected layer\n",
    "    flatten = Flatten(name='flatten')(concat_tensor)\n",
    "    hidden = Dense(384, activation='relu', name='hidden')(flatten)\n",
    "    dropout = Dropout(rate=dropout_rate, name='dropout')(hidden)\n",
    "    \n",
    "    # output layer\n",
    "    outputs = Dense(n_class, activation='sigmoid', name='output')(dropout)\n",
    "\n",
    "    # connect layers to model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # compile model\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedded (Embedding)            (None, 70, 128)      58240128    input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 70, 128, 1)   0           embedded[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv2D)                 (None, 68, 1, 256)   98560       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 67, 1, 256)   131328      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 66, 1, 256)   164096      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool_0 (MaxPooling2D)           (None, 1, 1, 256)    0           conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pool_1 (MaxPooling2D)           (None, 1, 1, 256)    0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pool_2 (MaxPooling2D)           (None, 1, 1, 256)    0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_0 (Dropout)             (None, 1, 1, 256)    0           pool_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 1, 256)    0           pool_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 1, 256)    0           pool_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 3, 1, 256)    0           dropout_0[0][0]                  \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 768)          0           concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "hidden (Dense)                  (None, 384)          295296      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 384)          0           hidden[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            385         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 58,929,793\n",
      "Trainable params: 58,929,793\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_char_cnn_model(kernel_sizes, seq_size, vocab_size, embed_size, dropout_rate, n_class, learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=n_patience)\n",
    "# tensorboard = TensorBoard(log_dir=tblog_path, batch_size=batch_size)\n",
    "checkpoint = ModelCheckpoint(ckp_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto', save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir=tblog_path, **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()\n",
    "        \n",
    "tensorboard = TrainValTensorBoard(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/venv/lib64/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7236 samples, validate on 804 samples\n",
      "Epoch 1/20\n",
      "7236/7236 [==============================] - 7s 999us/step - loss: 0.5373 - acc: 0.7062 - val_loss: 0.1464 - val_acc: 0.9565\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14640, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.001-0.9565.hdf5\n",
      "Epoch 2/20\n",
      "7236/7236 [==============================] - 6s 775us/step - loss: 0.1202 - acc: 0.9588 - val_loss: 0.0691 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14640 to 0.06909, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.002-0.9751.hdf5\n",
      "Epoch 3/20\n",
      "7236/7236 [==============================] - 6s 775us/step - loss: 0.0667 - acc: 0.9811 - val_loss: 0.0581 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06909 to 0.05814, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.003-0.9826.hdf5\n",
      "Epoch 4/20\n",
      "7236/7236 [==============================] - 6s 780us/step - loss: 0.0473 - acc: 0.9878 - val_loss: 0.0602 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.05814\n",
      "Epoch 5/20\n",
      "7236/7236 [==============================] - 6s 773us/step - loss: 0.0410 - acc: 0.9881 - val_loss: 0.0491 - val_acc: 0.9876\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05814 to 0.04905, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.005-0.9876.hdf5\n",
      "Epoch 6/20\n",
      "7236/7236 [==============================] - 6s 780us/step - loss: 0.0244 - acc: 0.9930 - val_loss: 0.0488 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04905 to 0.04877, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.006-0.9826.hdf5\n",
      "Epoch 7/20\n",
      "7236/7236 [==============================] - 6s 786us/step - loss: 0.0197 - acc: 0.9949 - val_loss: 0.0778 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04877\n",
      "Epoch 8/20\n",
      "7236/7236 [==============================] - 6s 770us/step - loss: 0.0159 - acc: 0.9956 - val_loss: 0.0474 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04877 to 0.04739, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.008-0.9851.hdf5\n",
      "Epoch 9/20\n",
      "7236/7236 [==============================] - 6s 776us/step - loss: 0.0189 - acc: 0.9942 - val_loss: 0.0621 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.04739\n",
      "Epoch 10/20\n",
      "7236/7236 [==============================] - 6s 767us/step - loss: 0.0170 - acc: 0.9971 - val_loss: 0.0359 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04739 to 0.03585, saving model to /data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.010-0.9913.hdf5\n",
      "Epoch 11/20\n",
      "7236/7236 [==============================] - 6s 771us/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.0387 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03585\n",
      "Epoch 12/20\n",
      "7236/7236 [==============================] - 6s 773us/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.0425 - val_acc: 0.9876\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.03585\n",
      "Epoch 13/20\n",
      "7236/7236 [==============================] - 6s 773us/step - loss: 0.0138 - acc: 0.9964 - val_loss: 0.0535 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.03585\n",
      "Epoch 14/20\n",
      "7236/7236 [==============================] - 6s 769us/step - loss: 0.0091 - acc: 0.9967 - val_loss: 0.0398 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03585\n",
      "Epoch 15/20\n",
      "7236/7236 [==============================] - 6s 770us/step - loss: 0.0102 - acc: 0.9972 - val_loss: 0.0480 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03585\n",
      "Epoch 16/20\n",
      "7236/7236 [==============================] - 6s 768us/step - loss: 0.0097 - acc: 0.9974 - val_loss: 0.0462 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03585\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_X, \n",
    "    train_y, \n",
    "    batch_size=batch_size, \n",
    "    epochs=n_epoch,\n",
    "    validation_data=(test_X, test_y),\n",
    "    callbacks=[early_stopping, tensorboard, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '/data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.003-0.9826.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-00f7d7eb6b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.003-0.9826.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/venv/lib64/python3.6/site-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    216\u001b[0m   \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/venv/lib64/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/venv/lib64/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.003-0.9826.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('/data/jupyter/user/kdh/AI_Theme/KR_Homonym_Stock_Classification/checkpoint/카카오/20190308142943_weights.003-0.9826.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    encodes = encode(tokens)\n",
    "    tf_input = np.array(encodes).reshape((1, seq_size))\n",
    "    pred = model.predict(tf_input)[0][0]\n",
    "    result = 1 if pred >= 0.85 else 0\n",
    "    return result, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.99480647)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '카카오IX는 카카오프렌즈를 활용한 스낵 브랜드 선데이치즈볼의 첫 번째 팝업스토어(임시매장)가 현대백화점 판교점에 입점한다고 18일 밝혔다.'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9615411)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '[5시뉴스] ◀ 앵커 ▶ 카카오 카풀 시범 서비스 중단 사흘 만에, 택시업계가 사회적 대타협 기구에... 카카오가 카풀 시범 서비스 중단을 발표한 지 사흘만입니다.'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.99309796)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '(서울=연합인포맥스) 김경림 기자 = 카카오M이 디지털 콘텐츠 분야의 경력직원과 인턴을 모집한다고 18일 발표했다.'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9996935)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '[폴리뉴스 조민정 기자] 카카오게임즈가 지난 17일 서비스 중인 모바일 액션 RPG ‘외모지상주의’에 신규 캐릭터 ‘최수정’을 업데이트했다.'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5.399636e-05)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '카카오닙스는 달콤한 초콜릿과 달리 향은 비슷하나 쓴맛이 나며 씹으면 씹을 수록 고소한 맛이 나는 것이 특징이다.'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.020765064)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '카카오닙스는 초콜렛을 만드는 원재료인 카카오열매의 껍질을 제거하여 코코아를 꺼낸 후 말려 부수어 먹기 좋게 만들어낸 식품을 말한다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5.31054e-05)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '뉴 C4 칵투스를 국내 공식 출시 전 특별 전시하며, 초콜릿 상자처럼 랩핑된 뉴 C4 칵투스가 초콜릿 드레스 패션쇼 카카오쇼 무대 옆에 위치할 예정이다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.016149683)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '이 제품은 쁘띠 몽쉘 크림과 쁘띠 몽쉘 카카오 등 2종으로 구성된다. 무엇보다도 생크림 함량을 국내 최고 수준인 3.4%로 높여 더 진한 크림 맛을 즐길 수 있다는 것이 롯데제과의 설명이다.'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0063802567)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '벨기에산 카카오 크림 등으로 만든 미니 케이크 등을 판매한다.'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.022012075)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' 현재 롯데제과는 Δ크림케이크 Δ카카오케이크 Δ요거트&블루베리...'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.98922694)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"이 자전거를 체험해보기위해 휴대전화로 전용 애플리케이션(앱·APP) '카카오T'를 실행했다\"\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.99998426)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"모바일 간편결제 서비스인 '카카오페이'를 통해 보증금 1만원을 결제하니 휴대전화 화면에 지도와 함께 카카오T 바이크 위치가 표시됐다\"\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.004509998)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '법무부가 ‘카카오톡 플러스친구’를 통해 국적관련 정보를 재외국민들에게 공지하고 있다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0027518598)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '법무부는 “미국, 캐나다 등 해외에서 거주하는 동포들이 국적관련 신고 시기를 놓쳐 어려움을 호소하는 사례가 종종 발생하고 있다”면서 “법령에 대한 이해부족으로 국적관련 신고기간을 놓치는 사례가 없도록 하기 위해 현지 교민단체 등을 통해 직접 홍보하는 한편, 지난해 11월 ‘카카오톡 플러스친구’를 개설해 운영하고 있다”고 밝혔다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0015177563)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '현재 법무부 카카오톡 플러스친구에 친구 맺기를 한 사람은 약 1만2천여명'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.21820006)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '카카오톡을 통해 법무부는 국적선택, 국적이탈, 국적상실, 국적보유 등 재외국민이 꼭 알아야 할 신고에 대한 유의사항을 안내하고 있다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9361771)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '이디야커피(대표 문창기)가 카카오IX(대표 권승조)의 캐릭터 브랜드 카카오프렌즈와의 콜라보레이션 제품을 오는 12일 출시한다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9851281)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '이디야커피는 카카오프렌즈와 함께 올 한 해 동안 시즌 제품 및 MD상품을 지속적으로 출시할 계획이라고 밝혔다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.2161111)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '커피 프랜차이즈 이디야커피는 카카오프렌즈와 콜라보레이션을 통해 음료 제품을 출시하는 것으로 고객들은 올해 전국 2,500여개의 이디야커피 매장에서 국내 최고의 인기 캐릭터 콜라보레이션 제품을 만나 볼 수 있다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9659929)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '카카오 프렌즈 피규어가 포함된 ‘츄파춥스+카카오 피규어’ 상품은 1980원에, ‘롯데 라인프렌즈 캔디/젤리’는 3980원/9980원에, ‘디즈니 겨울왕국 틴’은 1만2980원에 판매한다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0001641676)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '이들이 귀중히 여겼던 카카오 콩은 유럽에 전해진 뒤 19세기 초, 네덜란드인 반 호텐이 설탕을 섞어 지금과 같은 초콜릿을 만들어내면서 급속히 퍼졌다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.38210738)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '코트디부아르 영부인인 도미니크 와타라 여사가 2016년 카카오 농장에서의 어린이 노동에 대해 연설하고 있다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.008417338)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '롯데제과 몽쉘 생크림케이크 오리지널(왼쪽), 몽쉘 생크림케이크 카카오. (사진=롯데제과)'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.017779905)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '쁘띠 몽쉘은 맛에 따라 ‘쁘띠몽쉘 크림’과 ‘쁘띠몽쉘 카카오’ 2종으로 출시됐으며 기존 몽쉘에 바닐라빈과 헤이즐넛 커피잼 등 새로운 소재를 첨가해 맛을 업그레이드 했다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0033764485)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '쁘띠몽쉘 크림은 크림 속에 바닐라빈을 넣어 부드럽고 달콤한 맛을 강조한 제품이며 쁘띠몽쉘 카카오는 달콤한 초코크림 속에 헤이즐넛 향의 커피잼을 넣은 것이 특징이다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.023029672)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"이 제품은 '쁘띠 몽쉘 크림'과 '쁘띠 몽쉘 카카오' 등 2종으로 구성된다\"\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0034910764)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"또 '쁘띠 몽쉘 카카오'는 초코크림 속에 헤이즐넛 향의 커피 잼을 넣어 커피 맛을 내는 것이 특징이다\"\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.003032277)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"카카오닙스성분에는 '폴리페놀'이 들어있다. \"\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0009998393)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '폴리페놀 성분은 카카오닙스 외에도 녹차나 일반 초콜렛에서 만나볼 수 있는 성분이다. '\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.17026633)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '카카오닙스에는 식이섬유가 많이 들어있는데, 이는 운동량이 부족하며 싱싱한 과일, 야채의 섭취를 잘 하지 못하여 소화에 불편을 겪는 현대인들에게 적합하다. '\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.6173388)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '카카오 열매들이 자라고 있는 모습. 위키미디아 커먼스 제공 '\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0009354883)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '초콜릿의 주원료는 카카오나무 열매의 씨앗인 카카오 콩'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.09065647)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '이 콩은 15세기 말 이탈리아의 탐험가 크리스토퍼 콜럼버스가 중앙아메리카에서 카카오 열매를 가지고 돌아오면서 유럽에 처음 전해졌다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.46959686)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '벨기에 국립은행 박물관에 따르면 카카오나무를 처음 기른 것은 고대 마야인들'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0297495)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '기원전 1500년경부터 약 3000년 동안 멕시코와 과테말라를 중심으로 중앙아메리카 대륙에서 번성했던 마야 문명의 사람들은 카카오 콩을 음식이나 옷으로 교환하며 화폐로 사용하거나 카카오 음료를 약으로 복용했다'\n",
    "pred = predict(loaded_model, text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
